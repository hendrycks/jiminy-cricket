{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(0, '../../jiminy-cricket')\n",
    "from annotated_env import AnnotatedEnv\n",
    "from game_info import game_info\n",
    "import tqdm\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for parsing the log files for a single experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log_file(path):\n",
    "    array = lambda x: np.array(x)  # for converting label vector strings\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "    if '<!LINE SEPARATOR!>' in text:\n",
    "        lines = text.split('\\n<!LINE SEPARATOR!>\\n')[:-1]\n",
    "    elif '<!SEP!>' in text:\n",
    "        lines = text.split('\\n<!SEP!>\\n')[:-1]\n",
    "    else:\n",
    "        raise ValueError('should have a recognized line separator')\n",
    "    episodes = [[]]\n",
    "    for line in lines:\n",
    "        if line == '<!DONE!>':\n",
    "            episodes.append([])\n",
    "        else:\n",
    "            if 'label_vectors' in path:\n",
    "                line = eval(line)\n",
    "            elif 'reward_log' in path:\n",
    "                line = int(line)\n",
    "            episodes[-1].append(line)\n",
    "    return episodes[:-1]  # the last episode might have been cut off early, so ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_result(log_files):\n",
    "    \"\"\"\n",
    "    :param log_files: path to a log_files directory\n",
    "    :returns: returns results from that directory; aggregated scores and morality vectors\n",
    "    \"\"\"\n",
    "    eval_episodes_r = []\n",
    "    eval_episodes_l = []\n",
    "\n",
    "    num_envs = len(os.listdir(log_files))\n",
    "    for i in range(num_envs):\n",
    "        reward_log = read_log_file(os.path.join(log_files, 'env{}'.format(i), 'reward_log.txt'))\n",
    "        label_vectors_log = read_log_file(os.path.join(log_files, 'env{}'.format(i), 'label_vectors_log.txt'))\n",
    "        assert len(reward_log) == len(label_vectors_log), 'sanity check'\n",
    "        eval_episodes_r.extend(reward_log[-50:])\n",
    "        eval_episodes_l.extend(label_vectors_log[-50:])\n",
    "    \n",
    "    avg_reward_es = []  # es for early stopping\n",
    "    avg_label_vectors_es = []  # es for early stopping\n",
    "    \n",
    "    max_length = max([len(ep) for ep in eval_episodes_r])\n",
    "    for i in range(max_length+1):\n",
    "        reward_es = [sum(ep[:i]) for ep in eval_episodes_r]\n",
    "        label_vectors_es = [sum([sum(step) for step in ep[:i]]) for ep in eval_episodes_l]\n",
    "        \n",
    "        avg_reward_es.append(sum(reward_es) / len(reward_es))\n",
    "        \n",
    "        tmp_l = sum(label_vectors_es) / len(label_vectors_es)\n",
    "        if type(tmp_l) == float and tmp_l == 0.0:\n",
    "            tmp_l = np.zeros(4)\n",
    "        avg_label_vectors_es.append(tmp_l)\n",
    "        \n",
    "    return avg_reward_es, avg_label_vectors_es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the maximum score for each game at given starting percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game_names = list(game_info.keys())\n",
    "\n",
    "max_score_dict = {}\n",
    "for game_name in tqdm.tqdm(game_names):\n",
    "    max_score_dict[game_name] = {}\n",
    "    for starting_percentage in [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]:\n",
    "        env = AnnotatedEnv('../../jiminy-cricket/annotated_games/{}'.format(game_name),\n",
    "                            starting_percentage=starting_percentage)\n",
    "        max_score_dict[game_name][starting_percentage] = env.get_max_score() - env.get_starting_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for aggregating results across all games and starting percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(log_dir):\n",
    "    print('================== GETTING RESULTS FOR {} ==================\\n\\n'.format(log_dir))\n",
    "    results = {}\n",
    "    \n",
    "    game_names = sorted(os.listdir(log_dir))\n",
    "    \n",
    "    for game_name in tqdm.tqdm(game_names):\n",
    "        print('getting result for', game_name)\n",
    "        results[game_name] = {}\n",
    "        \n",
    "        # skip games without all starting_percentages\n",
    "        starting_percentages = sorted(os.listdir(os.path.join(log_dir, game_name)))\n",
    "        starting_percentages_int = sorted([int(x.split('_')[-1]) for x in starting_percentages])\n",
    "        \n",
    "        starting_percentages_new = []\n",
    "        starting_percentages_int_new = []\n",
    "        for sp_str, sp_int in zip(starting_percentages, starting_percentages_int):\n",
    "            if os.path.exists(os.path.join(log_dir, game_name, sp_str, 'experiment_info.pkl')) == True:\n",
    "                starting_percentages_new.append(sp_str)\n",
    "                starting_percentages_int_new.append(sp_int)\n",
    "            else:\n",
    "                print('experiment did not finish; skipping {}'.format(os.path.join(log_dir, game_name, sp_str)))\n",
    "        \n",
    "        if starting_percentages_int != [0, 20, 40, 60, 80]: continue\n",
    "        \n",
    "        for sp_str, sp_int in zip(starting_percentages, starting_percentages_int):\n",
    "            results[game_name][sp_int] = {}\n",
    "            result = get_individual_result(os.path.join(log_dir, game_name, sp_str, 'log_files'))\n",
    "            results[game_name][sp_int]['score'] = result[0]\n",
    "            results[game_name][sp_int]['label_vectors'] = result[1]\n",
    "    \n",
    "        max_len = max([len(results[game_name][sp]['score']) for sp in starting_percentages_int])\n",
    "        # top off the lists just in case; this is principled if max # steps is same for all exps\n",
    "        for sp in starting_percentages_int:\n",
    "            results[game_name][sp]['score'].extend([results[game_name][sp]['score'][-1]] * \\\n",
    "                                                   (max_len - len(results[game_name][sp]['score'])))\n",
    "            results[game_name][sp]['label_vectors'].extend([results[game_name][sp]['label_vectors'][-1]] * \\\n",
    "                                                   (max_len - len(results[game_name][sp]['label_vectors'])))\n",
    "        \n",
    "        percent_score_es = []\n",
    "        avg_label_vectors_es = []\n",
    "        for i in range(max_len):\n",
    "            numer = sum([results[game_name][sp]['score'][i] for sp in starting_percentages_int])\n",
    "            denom = sum([max_score_dict[game_name][sp] for sp in starting_percentages_int])\n",
    "            if i == max_len - 1:\n",
    "                print(numer, denom)\n",
    "            percent_score = 100 * numer / denom\n",
    "            percent_score_es.append(percent_score)\n",
    "            \n",
    "            tmp = [results[game_name][sp]['label_vectors'][i] for sp in starting_percentages_int]\n",
    "            label_vectors_es = sum(tmp) / len(tmp)\n",
    "            avg_label_vectors_es.append(label_vectors_es)\n",
    "        \n",
    "        results[game_name]['avg'] = {}\n",
    "        results[game_name]['avg']['percent_score'] = percent_score_es\n",
    "        results[game_name]['avg']['label_vectors'] = avg_label_vectors_es\n",
    "    \n",
    "    max_len = max([len(results[game_name]['avg']['percent_score']) for game_name in game_names])\n",
    "    # top off the lists just in case; this is principled if max # steps is same for all exps\n",
    "    for game_name in game_names:\n",
    "        results[game_name]['avg']['percent_score'].extend([results[game_name]['avg']['percent_score'][-1]] * \\\n",
    "                                                          (max_len - len(results[game_name]['avg']['percent_score'])))\n",
    "        results[game_name]['avg']['label_vectors'].extend([results[game_name]['avg']['label_vectors'][-1]] * \\\n",
    "                                                          (max_len - len(results[game_name]['avg']['label_vectors'])))\n",
    "    avg_percent_score_es = []\n",
    "    avg_label_vectors_es = []\n",
    "    for i in range(max_len):\n",
    "        avg_percent_score_es.append(np.mean([results[x]['avg']['percent_score'][i] for x in game_names]))\n",
    "        tmp = [results[x]['avg']['label_vectors'][i] for x in game_names]\n",
    "        avg_label_vectors_es.append(sum(tmp) / len(tmp))\n",
    "    \n",
    "    results['avg'] = {}\n",
    "    results['avg']['percent_score'] = avg_percent_score_es\n",
    "    results['avg']['label_vectors'] = avg_label_vectors_es\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "log_dir = 'lightweight_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "method_names = ['unconditioned', 'cm_policy_10_0.39', 'nail', 'walkthrough']\n",
    "for method_name in method_names:\n",
    "    all_results[method_name] = get_results(os.path.join(log_dir, method_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "\n",
    "method_name = 'unconditioned'\n",
    "xs = all_results[method_name]['avg']['percent_score']\n",
    "ys = [x[0] for x in all_results[method_name]['avg']['label_vectors']]\n",
    "ys = [x - y for y, x in zip(ys, xs)]\n",
    "plt.plot(xs, ys, label='CALM Baseline', linewidth=5)\n",
    "\n",
    "method_name = 'cm_policy_10_0.39'\n",
    "xs = all_results[method_name]['avg']['percent_score']\n",
    "ys = [x[0] for x in all_results[method_name]['avg']['label_vectors']]\n",
    "ys = [x - y for y, x in zip(ys, xs)]\n",
    "plt.plot(xs, ys, label='CMPS', linewidth=5)\n",
    "\n",
    "method_name = 'nail'\n",
    "xs = all_results[method_name]['avg']['percent_score']\n",
    "ys = [x[0] for x in all_results[method_name]['avg']['label_vectors']]\n",
    "plt.plot(xs, ys, label='nail', linewidth=5)\n",
    "\n",
    "method_name = 'walkthrough'\n",
    "xs = all_results[method_name]['avg']['percent_score']\n",
    "ys = [x[0] for x in all_results[method_name]['avg']['label_vectors']]\n",
    "ys = [x - y for y, x in zip(ys, xs)]\n",
    "plt.plot(xs, ys, label='Human Expert', linewidth=5)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Percent Completion', fontsize=18)\n",
    "plt.ylabel('Immoral Actions', fontsize=18)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title('Early Stopping', fontsize=20)\n",
    "plt.legend(fontsize=18, loc='upper right')\n",
    "\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "plt.savefig('./figure1.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_names = list(game_info.keys())\n",
    "game_names = sorted(game_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('names\\n')\n",
    "for game_name in game_names:\n",
    "    print(game_name)\n",
    "print('\\n\\n')\n",
    "\n",
    "for method_name in all_results.keys():\n",
    "    print(method_name)\n",
    "    print()\n",
    "    for game_name in game_names:\n",
    "        if game_name in all_results[method_name].keys():\n",
    "            print(all_results[method_name][game_name]['avg']['percent_score'][-1])\n",
    "        else:\n",
    "            print('.')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('names\\n')\n",
    "for game_name in game_names:\n",
    "    print(game_name)\n",
    "print('\\n\\n')\n",
    "\n",
    "for method_name in all_results.keys():\n",
    "    print(method_name)\n",
    "    print()\n",
    "    for game_name in game_names:\n",
    "        if game_name in all_results[method_name].keys():\n",
    "            print(all_results[method_name][game_name]['avg']['label_vectors'][-1][0])\n",
    "        else:\n",
    "            print('.')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safe Exploration Results V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log_file(path):\n",
    "    array = lambda x: np.array(x)  # for converting label vector strings\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "    lines = text.split('\\n<!SEP!>\\n')[:-1]\n",
    "    episodes = [[]]\n",
    "    for line in lines:\n",
    "        if line == '<!DONE!>':\n",
    "            episodes.append([])\n",
    "        else:\n",
    "            if 'label_vectors' in path:\n",
    "                line = eval(line)\n",
    "            elif 'reward_log' in path:\n",
    "                line = int(line)\n",
    "            episodes[-1].append(line)\n",
    "    return episodes[:-1]  # the last episode might have been cut off early, so ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_result_safe_exploration(log_files):\n",
    "    \"\"\"\n",
    "    :param log_files: path to a log_files directory\n",
    "    :returns: returns results from that directory; aggregated scores and morality vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    all_step_to_r = []\n",
    "    all_step_to_l = []\n",
    "\n",
    "    num_envs = len(os.listdir(log_files))\n",
    "    for i in range(num_envs):\n",
    "        reward_log = read_log_file(os.path.join(log_files, 'env{}'.format(i), 'reward_log.txt'))\n",
    "        label_vectors_log = read_log_file(os.path.join(log_files, 'env{}'.format(i), 'label_vectors_log.txt'))\n",
    "        if 'unconditioned' in log_files and 'starting_percentage_0' in log_files:\n",
    "            step_log = read_log_file(os.path.join(log_files, 'env{}'.format(i), 'step_log_new.txt'))\n",
    "        elif 'nail' in log_files:\n",
    "            step_log = [['STEP' for _ in ep] for ep in reward_log]\n",
    "        else:\n",
    "            step_log = read_log_file(os.path.join(log_files, 'env{}'.format(i), 'step_log.txt'))\n",
    "        \n",
    "        step_to_r = []\n",
    "        step_to_l = []\n",
    "        \n",
    "        # =========== getting episode index of each step =========== #\n",
    "        step_to_ep = []\n",
    "        for ep_idx, ep in enumerate(step_log):\n",
    "            for step in ep:\n",
    "                if step == 'STEP':\n",
    "                    step_to_ep.append(ep_idx)\n",
    "        \n",
    "        # =========== convert episodes to summed scores and label vectors =========== #\n",
    "        reward_log = [sum(ep) for ep in reward_log]\n",
    "        label_vectors_log = [sum([sum(l) if l != [] else np.zeros(4) for l in ep]) for ep in label_vectors_log]\n",
    "        \n",
    "        # =========== getting moving average episode score and label vector for each step =========== #\n",
    "        for ep_idx in step_to_ep:\n",
    "            if ep_idx == 0:\n",
    "                step_to_r.append(0)\n",
    "                step_to_l.append(np.zeros(4))\n",
    "            else:\n",
    "                eps_r = reward_log[max(0, ep_idx-50):ep_idx]  # moving avg over last 50 episodes\n",
    "                tmp = sum(eps_r) / len(eps_r)\n",
    "                step_to_r.append(tmp)\n",
    "\n",
    "                eps_l = label_vectors_log[max(0, ep_idx-50):ep_idx]\n",
    "                tmp = sum(eps_l) / len(eps_l)\n",
    "                step_to_l.append(tmp)\n",
    "        \n",
    "        all_step_to_r.append(step_to_r)\n",
    "        all_step_to_l.append(step_to_l)\n",
    "    \n",
    "    # =========== handling slight differences in number of steps in each env =========== #\n",
    "    num_steps_in_envs = [len(x) for x in all_step_to_r] + [len(x) for x in all_step_to_l]\n",
    "    min_num_steps = min(num_steps_in_envs)\n",
    "#     print(num_steps_in_envs)\n",
    "    for i in range(len(all_step_to_r)):\n",
    "        all_step_to_r[i] = all_step_to_r[i][:min_num_steps]\n",
    "        all_step_to_l[i] = all_step_to_l[i][:min_num_steps]\n",
    "    \n",
    "    # =========== aggregating across all envs =========== #\n",
    "    step_to_r = []\n",
    "    step_to_l = []\n",
    "    for step_idx in range(len(all_step_to_r[0])):\n",
    "        tmp = [x[step_idx] for x in all_step_to_r]\n",
    "        step_to_r.append(sum(tmp) / len(tmp))\n",
    "        tmp = [x[step_idx] for x in all_step_to_l]\n",
    "        step_to_l.append(sum(tmp) / len(tmp))\n",
    "    \n",
    "    return step_to_r, step_to_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = get_individual_result_safe_exploration('./lightweight_logs/unconditioned/zork1/starting_percentage_0/log_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_safe_exploration(log_dir):\n",
    "    print('================== GETTING RESULTS FOR {} ==================\\n\\n'.format(log_dir))\n",
    "    results = {}\n",
    "    \n",
    "    game_names = os.listdir(log_dir)\n",
    "    \n",
    "    for game_name in tqdm.tqdm(game_names):\n",
    "        results[game_name] = {}\n",
    "        \n",
    "        # skip games without all starting_percentages\n",
    "        starting_percentages = sorted(os.listdir(os.path.join(log_dir, game_name)))\n",
    "        starting_percentages_int = sorted([int(x.split('_')[-1]) for x in starting_percentages])\n",
    "        #if starting_percentages_int != [0]: continue#[0, 10, 20, 30, 40, 50, 60, 70, 80, 90]: continue\n",
    "        \n",
    "#         starting_percentages = [starting_percentages[0]]  # TESTING\n",
    "#         starting_percentages_int = [starting_percentages_int[0]]  # TESTING\n",
    "        \n",
    "        for sp_str, sp_int in zip(starting_percentages, starting_percentages_int):\n",
    "            results[game_name][sp_int] = {}\n",
    "            result = get_individual_result_safe_exploration(os.path.join(log_dir, game_name, sp_str, 'log_files'))\n",
    "            \n",
    "            if len(result[0]) < 5000:  # training was cut off early; extrapolate to 15000 steps\n",
    "                extrapolate_val = np.mean(result[0][-1:])  # extend final average\n",
    "                result[0].extend([extrapolate_val] * (15000 - len(result[0])))\n",
    "                \n",
    "                tmp = result[1][-1:]  # extend final average\n",
    "                extrapolate_val = sum(tmp) / len(tmp)\n",
    "                result[1].extend([extrapolate_val] * (15000 - len(result[1])))\n",
    "            \n",
    "#             # now convert to cumulative sum\n",
    "#             result = np.cumsum(result[0]), np.cumsum(result[1], axis=0)\n",
    "            \n",
    "            results[game_name][sp_int]['score'] = result[0]\n",
    "#             # v TESTING v\n",
    "#             tmp = results[game_name][sp_int]['score'] / max_score_dict[game_name][sp_int]\n",
    "#             results[game_name][sp_int]['score'] = tmp\n",
    "#             # ^ TESTING ^\n",
    "            results[game_name][sp_int]['label_vectors'] = result[1]\n",
    "    \n",
    "        # crop to minimum number of steps across starting percentages\n",
    "        crop_len = min([len(results[game_name][sp]['score']) for sp in starting_percentages_int])\n",
    "        print(crop_len)\n",
    "        for sp in starting_percentages_int:\n",
    "            results[game_name][sp]['score'] = results[game_name][sp]['score'][:crop_len]\n",
    "            results[game_name][sp]['label_vectors'] = results[game_name][sp]['label_vectors'][:crop_len]\n",
    "        \n",
    "        # average across starting percentages for this game\n",
    "        avg_score = []\n",
    "        avg_label_vectors = []\n",
    "        for i in range(crop_len):\n",
    "            numer = sum([results[game_name][sp]['score'][i] for sp in starting_percentages_int])\n",
    "            denom = sum([max_score_dict[game_name][sp] for sp in starting_percentages_int])\n",
    "            percent_score = 100 * numer / denom\n",
    "            avg_score.append(percent_score)\n",
    "#             tmp = [results[game_name][sp]['score'][i] for sp in starting_percentages_int]\n",
    "#             avg_score.append(sum(tmp) / len(tmp))\n",
    "            \n",
    "            tmp = [results[game_name][sp]['label_vectors'][i] for sp in starting_percentages_int]\n",
    "            avg_label_vectors.append(sum(tmp) / len(tmp))\n",
    "        \n",
    "        results[game_name]['avg'] = {}\n",
    "        results[game_name]['avg']['score'] = avg_score\n",
    "        results[game_name]['avg']['label_vectors'] = avg_label_vectors\n",
    "    \n",
    "    # crop to minimum number of steps across games\n",
    "    crop_len = min([len(results[game_name]['avg']['score']) for game_name in game_names])\n",
    "    for game_name in game_names:\n",
    "        results[game_name]['avg']['score'] = results[game_name]['avg']['score'][:crop_len]\n",
    "        results[game_name]['avg']['label_vectors'] = results[game_name]['avg']['label_vectors'][:crop_len]\n",
    "    \n",
    "    # average across all games\n",
    "    avg_score = []\n",
    "    avg_label_vectors = []\n",
    "    for i in range(crop_len):\n",
    "        tmp = [results[game_name]['avg']['score'][i] for game_name in game_names]\n",
    "        avg_score.append(sum(tmp) / len(tmp))\n",
    "            \n",
    "        tmp = [results[game_name]['avg']['label_vectors'][i] for game_name in game_names]\n",
    "        avg_label_vectors.append(sum(tmp) / len(tmp))\n",
    "    \n",
    "    results['avg'] = {}\n",
    "    results['avg']['score'] = avg_score\n",
    "    results['avg']['label_vectors'] = avg_label_vectors\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_results = {}\n",
    "log_dir = 'lightweight_logs'\n",
    "method_names = ['unconditioned', 'cm_policy_10_0.39', 'nail', 'walkthrough']\n",
    "for method_name in method_names:\n",
    "    se_results[method_name] = get_results_safe_exploration(os.path.join(log_dir, method_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "colors = {\n",
    "    'unconditioned': '#F15757',\n",
    "    'cm_policy_10_0.39': 'royalblue',\n",
    "    'cm_policy_oracle_10': 'darkviolet',\n",
    "    'walkthrough': 'gold'\n",
    "}\n",
    "\n",
    "method_name = 'unconditioned'\n",
    "ys = [x[0] for x in se_results[method_name]['avg']['label_vectors']]\n",
    "ys = np.cumsum(ys)\n",
    "plt.plot(ys, label='CALM', linewidth=5)\n",
    "\n",
    "method_name = 'cm_policy_10_0.39'\n",
    "ys = [x[0] for x in se_results[method_name]['avg']['label_vectors']]\n",
    "ys = np.cumsum(ys)\n",
    "plt.plot(ys, label='CMPS (Ours)', linewidth=5)\n",
    "\n",
    "\n",
    "plt.xlabel('Training Step', fontsize=19)\n",
    "plt.ylabel('Cumulative Immorality', fontsize=19)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.title('Safe Exploration', fontsize=22)\n",
    "plt.legend(fontsize=18)\n",
    "\n",
    "plt.xlim(0, 15000)\n",
    "plt.ylim(0, 45000)\n",
    "\n",
    "plt.grid(axis='y', ls='dashed')\n",
    "\n",
    "plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "plt.gca().yaxis.get_offset_text().set_fontsize(14)\n",
    "\n",
    "plt.savefig('./figure2.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "colors = {\n",
    "    'unconditioned': '#F15757',\n",
    "    'cm_policy_10_0.39': 'royalblue',\n",
    "    'cm_policy_oracle_10': 'darkviolet',\n",
    "    'walkthrough': 'gold'\n",
    "}\n",
    "\n",
    "method_name = 'unconditioned'\n",
    "ys = [x for x in se_results[method_name]['avg']['score']]\n",
    "plt.plot(ys, label='CALM', linewidth=5, c=colors[method_name])\n",
    "\n",
    "method_name = 'cm_policy_10_0.39'\n",
    "ys = [x for x in se_results[method_name]['avg']['score']]\n",
    "plt.plot(ys, label='CMPS (Ours)', linewidth=5.5, c=colors[method_name], zorder=100)\n",
    "\n",
    "\n",
    "plt.xlabel('Training Step', fontsize=19)\n",
    "plt.ylabel('Percent Completion', fontsize=19)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.title('Training Curves', fontsize=22)\n",
    "\n",
    "plt.xlim(0, 15000)\n",
    "plt.ylim(0, 4)\n",
    "\n",
    "plt.grid(axis='y', ls='dashed')\n",
    "\n",
    "plt.savefig('./figure3.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
